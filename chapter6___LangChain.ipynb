{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fe9afc3-637c-44d0-b5f2-4eccaeabeb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain==0.0.181\n",
    "# !pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca988c2-338f-4029-8240-420757246590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac320ad1-78f2-43c2-a86c-6aecb8942b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f02dc789-6a9c-46f0-9330-f4a234a110b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d4075d5-7380-44a4-ac34-1a92b75e287b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "\n",
      "헤픈게이밍\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(temperature = 0.9)\n",
    "print(llm('컴퓨터 게임을 만드는 새로운 한국어 회사명을 하나 제안해 주세요'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b7b16b8-d3dc-4d1c-b0be-56255d06b9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가정용 로봇을 만드는 새로운 한국어 회사명을 하나 제안해 주세요.\n"
     ]
    }
   ],
   "source": [
    "# prompt template\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "                        input_variables = ['product'],\n",
    "                        template = '{product}을 만드는 새로운 한국어 회사명을 하나 제안해 주세요.'\n",
    ")\n",
    "\n",
    "print(prompt.format(product = '가정용 로봇'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78c04ef6-5456-4d3c-927a-2a7c96e8f150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.\\n\\n나랑로봇'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt template\n",
    "prompt = PromptTemplate(\n",
    "                        input_variables = ['product'],\n",
    "                        template = '{product}을 만드는 새로운 한국어 회사명을 하나 제안해주세요'\n",
    ")\n",
    "\n",
    "# chain\n",
    "chain = LLMChain(\n",
    "                    llm = OpenAI(temperature = 0.9),\n",
    "                    prompt = prompt\n",
    ")\n",
    "\n",
    "# run\n",
    "chain.run('가정용 로봇')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed3b405f-40a5-407f-86ae-4a49d3b3d3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "chain = ConversationChain(\n",
    "                            llm = OpenAI(temperature = 0),\n",
    "                            verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd08805c-4e43-466e-951d-13e27dd4ce25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: 우리집 반려견 이름은 보리입니다.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' 반갑습니다, 보리씨! 보리는 어떤 견종인가요?'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run('우리집 반려견 이름은 보리입니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95203d5e-1fcf-4acc-9161-37105fc0b912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: 우리집 반려견 이름은 보리입니다.\n",
      "AI:  반갑습니다, 보리씨! 보리는 어떤 견종인가요?\n",
      "Human: 우리집 반려견 이름을 불러주세요\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' 보리를 불러주겠습니다! \"보리, 여기 와라!\"'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run('우리집 반려견 이름을 불러주세요')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "803dfc2e-fc72-4690-83db-d329e6313970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: 우리집 반려견 이름은 보리입니다.\n",
      "AI:  반갑습니다, 보리씨! 보리는 어떤 견종인가요?\n",
      "Human: 우리집 반려견 이름을 불러주세요\n",
      "AI:  보리를 불러주겠습니다! \"보리, 여기 와라!\"\n",
      "Human: 우리집 반려견 이름을 불러주세요\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' 보리를 불러주겠습니다! \"보리, 여기 와라!\"'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(input = '우리집 반려견 이름을 불러주세요')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4db7546-3931-4605-9356-a2625db469e9",
   "metadata": {},
   "source": [
    "# LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8edaafa1-4ddc-4aeb-862e-c048ec59986e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    model_name : OpenAI API의 모델명\\n    max_tokens : 최대 출력 토큰수\\n    temperature : 무작위성\\n    n : 생성할 결과 수\\n    cache : 캐시 활성화/비활성화 선택\\n    streaming : 스트리밍 활성화/비활성화 선택\\n    callback_manager : 콜백 매니저\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = OpenAI(\n",
    "                model_name = 'text-davinci-003',\n",
    "                temperature = 0\n",
    ")\n",
    "\n",
    "# 주요매개변수\n",
    "'''\n",
    "    model_name : OpenAI API의 모델명\n",
    "    max_tokens : 최대 출력 토큰수\n",
    "    temperature : 무작위성\n",
    "    n : 생성할 결과 수\n",
    "    cache : 캐시 활성화/비활성화 선택\n",
    "    streaming : 스트리밍 활성화/비활성화 선택\n",
    "    callback_manager : 콜백 매니저\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc9da128-2967-4318-9cc7-96408b78b9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "고양이 울음소리는 \"야옹\"으로 나타납니다.\n"
     ]
    }
   ],
   "source": [
    "result = llm('고양이 울음소리는?')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc0a1556-4c8a-4f60-ad74-3165ce8aac1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response: \n",
      "\n",
      "고양이 울음소리는 \"야옹\"으로 나타납니다.\n",
      "response: \n",
      "\n",
      "까마귀의 울음소리는 \"까악까악\"이라고 합니다.\n",
      "\n",
      "llm_output: {'token_usage': {'prompt_tokens': 47, 'total_tokens': 156, 'completion_tokens': 109}, 'model_name': 'text-davinci-003'}\n"
     ]
    }
   ],
   "source": [
    "# 고급 llm 호출\n",
    "result = llm.generate(['고양이 울음소리는?', '까마귀 울음소리는?'])\n",
    "\n",
    "print('response:', result.generations[0][0].text)\n",
    "print('response:', result.generations[1][0].text)\n",
    "\n",
    "print('\\nllm_output:', result.llm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e3a7c005-aa89-4c42-ba26-f70a97da79ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "chat_llm = ChatOpenAI(\n",
    "                        model_name = 'gpt-3.5-turbo',\n",
    "                        temperature = 0\n",
    ")\n",
    "\n",
    "messages = [HumanMessage(content = '고양이 울음소리는?')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d2c247e3-85d6-4289-82ea-c0cb09cda147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='고양이의 울음소리는 \"야옹\"이라고 표현됩니다.'\n"
     ]
    }
   ],
   "source": [
    "result = chat_llm(messages)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3d39a3ea-31fd-492e-91e2-7feee54a8929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "고양이의 울음소리는 \"야옹\"이라고 표현됩니다.\n",
      "까악까악\n",
      "{'token_usage': {'completion_tokens': 34, 'prompt_tokens': 38, 'total_tokens': 72}, 'model_name': 'gpt-3.5-turbo'}\n"
     ]
    }
   ],
   "source": [
    "messages_list = [\n",
    "    [HumanMessage(content = '고양이 울음소리는?')],\n",
    "    [HumanMessage(content = '까마귀 울음소리는?')]\n",
    "]\n",
    "\n",
    "result = chat_llm.generate(messages_list)\n",
    "\n",
    "print(result.generations[0][0].text)\n",
    "print(result.generations[1][0].text)\n",
    "\n",
    "print(result.llm_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7aa7472-cb54-4208-a97d-080f7b15fe88",
   "metadata": {},
   "source": [
    "### Cache(캐시)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9e6241c1-4dec-41c2-8f20-03c3c238cd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain.cache import InMemoryCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "302d164e-a484-4e82-92db-d0d5d954336e",
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain.llm_cache = InMemoryCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2c40c550-8009-4f44-9b0b-46fc77f59192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[Generation(text='\\n\\n하늘의 색깔은 파랑색입니다.', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'prompt_tokens': 16, 'total_tokens': 51, 'completion_tokens': 35}, 'model_name': 'text-davinci-003'}, run=[RunInfo(run_id=UUID('92ac33ab-aa9b-461f-87fa-1bfdcfa255c3'))])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.generate(['하늘의 색깔은?']) # llm_output 정보가 있는 것을 통해 API를 호출하여 결과를 작성했음을 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f23cbd89-f4f7-4509-8067-e5065a5a7387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[Generation(text='\\n\\n하늘의 색깔은 파랑색입니다.', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={}, run=None)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.generate(['하늘의 색깔은?']) # llm_output이 비어있다 >>> API 호출 없이 캐시에서 결과를 읽어온 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a9ba773d-7d6c-4509-a5ba-c06380c8dfc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[Generation(text='\\n\\n하늘의 색깔은 파랑입니다.', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'prompt_tokens': 16, 'total_tokens': 48, 'completion_tokens': 32}, 'model_name': 'text-davinci-003'}, run=[RunInfo(run_id=UUID('3b25a5cd-252e-4fcc-8627-998a79af7871'))])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = OpenAI(cache = False)\n",
    "\n",
    "llm.generate(['하늘의 색깔은?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "33638b83-9a18-42b2-999e-99e459c5cef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[Generation(text='\\n\\n하늘의 색깔은 밝은 파란색이라고 할 수 있습니다.', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'prompt_tokens': 16, 'total_tokens': 73, 'completion_tokens': 57}, 'model_name': 'text-davinci-003'}, run=[RunInfo(run_id=UUID('1587f8f8-1773-4b72-968d-44607a024d95'))])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 전역 캐시 비활성화\n",
    "langchain.llm_cache = None\n",
    "\n",
    "llm.generate(['하늘의 색깔은?'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cfb174-91f0-4be0-b216-6e382082d4d6",
   "metadata": {},
   "source": [
    "### 비동기 처리\n",
    "- 동기식 처리: 프로그램이 하나의 작업을 완료할 때까지 다른 작업을 시작하지 않는 방식\n",
    "- 비동기식 처리: 프로긂이 작업을 시작함과 동시에 여러 작업을 같이 시작하는 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "05cb9855-a9a3-49e4-adaa-6710f2bf9222",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 동기화 처리시에 소요되는 시간 확인\n",
    "import time\n",
    "\n",
    "def generate_serially():\n",
    "    llm = OpenAI(temperature = 0.9)\n",
    "    \n",
    "    for _ in range(10):\n",
    "        resp = llm.generate(['안녕하세요'])\n",
    "        print(resp.generations[0][0].text)\n",
    "        \n",
    "\n",
    "s = time.perf_counter()\n",
    "\n",
    "generate_serially()\n",
    "\n",
    "elasped = time.perf_counter() - s\n",
    "print(f'{elasped:.2f}초')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "443f7c24-2e4c-400f-88c3-202e175f5ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비동기화 처리\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def async_generate(llm):\n",
    "    resp = await llm.agenerate(['안녕하세요!'])\n",
    "    print(resp.generations[0][0].text)\n",
    "    \n",
    "async def generate_concurrently():\n",
    "    llm = OpenAI(temperature = 0.9)\n",
    "    tasks = [async_generate(llm) for _ in range(10)]\n",
    "    await asyncio.gather(*task)\n",
    "    \n",
    "s = time.perf_counter()\n",
    "\n",
    "asyncio.run(generate_concurently())\n",
    "\n",
    "elasped = time.perf_counter() - s\n",
    "print(f'{elasped:.2f}초')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d88642c-59d2-43ad-ac14-4a7ca1c02e06",
   "metadata": {},
   "source": [
    "### 스트리밍\n",
    "\n",
    "LLM의 스트리밍은 한 번에 모두 출려하지 않고, 토큰 단위로 출력을 돌려줌으로써 체감하는 대기 시간을 줄이는 기능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "97b0c938-5210-41e4-a53d-5481ed22e845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "내 생활이 바뀌었어 내 친구는 ChatGPT\n",
      "말하고 듣고 즐거워 내 이야기는 다 담기네\n",
      "내 생각을 말하고 내 마음을 들어줘\n",
      "내 이야기는 다 담기네\n",
      "\n",
      "내 생활이 바뀌었어 내 친구는 ChatGPT\n",
      "생각을 나누고 말하고 즐거워 내 이야기는 다 담"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "llm = OpenAI(\n",
    "                streaming = True,\n",
    "                callbacks = [StreamingStdOutCallbackHandler()],\n",
    "                verbose = True,\n",
    "                temperature = 0\n",
    ")\n",
    "\n",
    "resp = llm('즐거운 ChatGPT 생활을 가사로 만들어 주세요.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4167b2b-681f-467a-acdf-d40485edf12e",
   "metadata": {},
   "source": [
    "# Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6fd578ee-bb32-478b-b05f-aa02af0a414c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.5.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.9/site-packages (from tiktoken) (2023.3.23)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.9/site-packages (from tiktoken) (2.28.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Installing collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.5.1\n",
      "Collecting faiss-gpu\n",
      "  Downloading faiss_gpu-1.7.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-gpu\n",
      "Successfully installed faiss-gpu-1.7.2\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken\n",
    "!pip install faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2c8cf232-71ee-4850-853f-4d3a1725abeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ccae3575-a473-4e40-8e83-b6ec633708f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "멋진 동물이라고 하면?\n"
     ]
    }
   ],
   "source": [
    "no_input_prompt = PromptTemplate(input_variables = [],\n",
    "                                 template = '멋진 동물이라고 하면?')\n",
    "\n",
    "print(no_input_prompt.format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5ba3505b-3530-4e02-b0f8-3d9c078c4dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "멋진 동물이라고 하면?\n"
     ]
    }
   ],
   "source": [
    "# 하나의 입력 변수가 있는 프롬프트 템플릿\n",
    "one_input_prompt = PromptTemplate(input_variables = ['content'],\n",
    "                                  template = '멋진 {content}이라고 하면?')\n",
    "\n",
    "print(one_input_prompt.format(content = '동물'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0321f091-64e0-4a9f-8b91-28369df76bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "멋진 동물이라고 하면?\n"
     ]
    }
   ],
   "source": [
    "# 여러개의 입력 변수가 있는 프롬프트 템플릿\n",
    "multiple_inpurt_prompt = PromptTemplate(input_variables = ['adjective', 'content'],\n",
    "                                        template = '{adjective} {content}이라고 하면?')\n",
    "\n",
    "print(multiple_inpurt_prompt.format(adjective = '멋진', content = '동물'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
